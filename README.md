# XGBoost-Algorithm-Implementation

**Overview**

This project introduces the Extreme Gradient Boosting (XGBoost) algorithm, focusing on its application in classification using decision trees. XGBoost enhances prediction accuracy by building trees sequentially, where each tree corrects errors from the previous one. The tutorial covers the algorithm, tuning parameters, and a comparison with Random Forest models.

**Key Topics**

* Boosting Basics: Explains boosting as a sequential decision tree algorithm to reduce residual errors.
* XGBoost: A faster, regularized form of gradient boosting with better scalability for large datasets.
* Tuning Parameters: Overview of key parameters like the number of trees, learning rate, max depth, and gamma.
* Advantages & Disadvantages: XGBoost's strengths in handling data imbalance and real-time data vs. its complexity and risk of overfitting.
* Model Comparison: Evaluates XGBoost against Random Forest, with a focus on log loss metrics for model accuracy.

**Evaluation**

The project demonstrates that while XGBoost is powerful, Random Forest performed better for the provided dataset due to lower log loss. However, XGBoost may offer advantages for datasets with class imbalances or specific applications needing misclassification improvement.

_This project was part of a final group assignment for a college course._
